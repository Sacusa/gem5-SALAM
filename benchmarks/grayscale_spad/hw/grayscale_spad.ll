; ModuleID = 'grayscale_spad.bc'
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: norecurse nounwind uwtable
define void @grayscale(i64 %num_elems) #0 {
entry:
  br label %for.body

for.body:                                         ; preds = %for.body, %entry
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next.31, %for.body ]
  %0 = mul nuw nsw i64 %indvars.iv, 3
  %arrayidx = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %0
  %1 = load volatile i8, i8* %arrayidx, align 16
  %2 = or i64 %0, 1
  %arrayidx3 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %2
  %3 = load volatile i8, i8* %arrayidx3, align 1
  %4 = or i64 %0, 2
  %arrayidx6 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %4
  %5 = load volatile i8, i8* %arrayidx6, align 2
  %conv8 = uitofp i8 %1 to double
  %mul9 = fmul double %conv8, 2.126000e-01
  %conv11 = uitofp i8 %3 to double
  %mul12 = fmul double %conv11, 7.152000e-01
  %add13 = fadd double %mul9, %mul12
  %conv15 = uitofp i8 %5 to double
  %mul16 = fmul double %conv15, 7.220000e-02
  %add17 = fadd double %add13, %mul16
  %conv18 = fptoui double %add17 to i8
  %conv19 = uitofp i8 %conv18 to float
  %arrayidx21 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv
  store volatile float %conv19, float* %arrayidx21, align 16
  %indvars.iv.next = or i64 %indvars.iv, 1
  %6 = mul nuw nsw i64 %indvars.iv.next, 3
  %arrayidx.1 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %6
  %7 = load volatile i8, i8* %arrayidx.1, align 1
  %8 = add nuw nsw i64 %6, 1
  %arrayidx3.1 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %8
  %9 = load volatile i8, i8* %arrayidx3.1, align 1
  %10 = add nuw nsw i64 %6, 2
  %arrayidx6.1 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %10
  %11 = load volatile i8, i8* %arrayidx6.1, align 1
  %conv8.1 = uitofp i8 %7 to double
  %mul9.1 = fmul double %conv8.1, 2.126000e-01
  %conv11.1 = uitofp i8 %9 to double
  %mul12.1 = fmul double %conv11.1, 7.152000e-01
  %add13.1 = fadd double %mul9.1, %mul12.1
  %conv15.1 = uitofp i8 %11 to double
  %mul16.1 = fmul double %conv15.1, 7.220000e-02
  %add17.1 = fadd double %add13.1, %mul16.1
  %conv18.1 = fptoui double %add17.1 to i8
  %conv19.1 = uitofp i8 %conv18.1 to float
  %arrayidx21.1 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next
  store volatile float %conv19.1, float* %arrayidx21.1, align 4
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %12 = mul nuw nsw i64 %indvars.iv.next.1, 3
  %arrayidx.2 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %12
  %13 = load volatile i8, i8* %arrayidx.2, align 2
  %14 = or i64 %12, 1
  %arrayidx3.2 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %14
  %15 = load volatile i8, i8* %arrayidx3.2, align 1
  %16 = add nuw nsw i64 %12, 2
  %arrayidx6.2 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %16
  %17 = load volatile i8, i8* %arrayidx6.2, align 2
  %conv8.2 = uitofp i8 %13 to double
  %mul9.2 = fmul double %conv8.2, 2.126000e-01
  %conv11.2 = uitofp i8 %15 to double
  %mul12.2 = fmul double %conv11.2, 7.152000e-01
  %add13.2 = fadd double %mul9.2, %mul12.2
  %conv15.2 = uitofp i8 %17 to double
  %mul16.2 = fmul double %conv15.2, 7.220000e-02
  %add17.2 = fadd double %add13.2, %mul16.2
  %conv18.2 = fptoui double %add17.2 to i8
  %conv19.2 = uitofp i8 %conv18.2 to float
  %arrayidx21.2 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.1
  store volatile float %conv19.2, float* %arrayidx21.2, align 8
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %18 = mul nuw nsw i64 %indvars.iv.next.2, 3
  %arrayidx.3 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %18
  %19 = load volatile i8, i8* %arrayidx.3, align 1
  %20 = add nuw nsw i64 %18, 1
  %arrayidx3.3 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %20
  %21 = load volatile i8, i8* %arrayidx3.3, align 1
  %22 = add nuw nsw i64 %18, 2
  %arrayidx6.3 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %22
  %23 = load volatile i8, i8* %arrayidx6.3, align 1
  %conv8.3 = uitofp i8 %19 to double
  %mul9.3 = fmul double %conv8.3, 2.126000e-01
  %conv11.3 = uitofp i8 %21 to double
  %mul12.3 = fmul double %conv11.3, 7.152000e-01
  %add13.3 = fadd double %mul9.3, %mul12.3
  %conv15.3 = uitofp i8 %23 to double
  %mul16.3 = fmul double %conv15.3, 7.220000e-02
  %add17.3 = fadd double %add13.3, %mul16.3
  %conv18.3 = fptoui double %add17.3 to i8
  %conv19.3 = uitofp i8 %conv18.3 to float
  %arrayidx21.3 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.2
  store volatile float %conv19.3, float* %arrayidx21.3, align 4
  %indvars.iv.next.3 = or i64 %indvars.iv, 4
  %24 = mul nuw nsw i64 %indvars.iv.next.3, 3
  %arrayidx.4 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %24
  %25 = load volatile i8, i8* %arrayidx.4, align 4
  %26 = or i64 %24, 1
  %arrayidx3.4 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %26
  %27 = load volatile i8, i8* %arrayidx3.4, align 1
  %28 = or i64 %24, 2
  %arrayidx6.4 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %28
  %29 = load volatile i8, i8* %arrayidx6.4, align 2
  %conv8.4 = uitofp i8 %25 to double
  %mul9.4 = fmul double %conv8.4, 2.126000e-01
  %conv11.4 = uitofp i8 %27 to double
  %mul12.4 = fmul double %conv11.4, 7.152000e-01
  %add13.4 = fadd double %mul9.4, %mul12.4
  %conv15.4 = uitofp i8 %29 to double
  %mul16.4 = fmul double %conv15.4, 7.220000e-02
  %add17.4 = fadd double %add13.4, %mul16.4
  %conv18.4 = fptoui double %add17.4 to i8
  %conv19.4 = uitofp i8 %conv18.4 to float
  %arrayidx21.4 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.3
  store volatile float %conv19.4, float* %arrayidx21.4, align 16
  %indvars.iv.next.4 = or i64 %indvars.iv, 5
  %30 = mul nuw nsw i64 %indvars.iv.next.4, 3
  %arrayidx.5 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %30
  %31 = load volatile i8, i8* %arrayidx.5, align 1
  %32 = add nuw nsw i64 %30, 1
  %arrayidx3.5 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %32
  %33 = load volatile i8, i8* %arrayidx3.5, align 1
  %34 = add nuw nsw i64 %30, 2
  %arrayidx6.5 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %34
  %35 = load volatile i8, i8* %arrayidx6.5, align 1
  %conv8.5 = uitofp i8 %31 to double
  %mul9.5 = fmul double %conv8.5, 2.126000e-01
  %conv11.5 = uitofp i8 %33 to double
  %mul12.5 = fmul double %conv11.5, 7.152000e-01
  %add13.5 = fadd double %mul9.5, %mul12.5
  %conv15.5 = uitofp i8 %35 to double
  %mul16.5 = fmul double %conv15.5, 7.220000e-02
  %add17.5 = fadd double %add13.5, %mul16.5
  %conv18.5 = fptoui double %add17.5 to i8
  %conv19.5 = uitofp i8 %conv18.5 to float
  %arrayidx21.5 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.4
  store volatile float %conv19.5, float* %arrayidx21.5, align 4
  %indvars.iv.next.5 = or i64 %indvars.iv, 6
  %36 = mul nuw nsw i64 %indvars.iv.next.5, 3
  %arrayidx.6 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %36
  %37 = load volatile i8, i8* %arrayidx.6, align 2
  %38 = or i64 %36, 1
  %arrayidx3.6 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %38
  %39 = load volatile i8, i8* %arrayidx3.6, align 1
  %40 = add nuw nsw i64 %36, 2
  %arrayidx6.6 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %40
  %41 = load volatile i8, i8* %arrayidx6.6, align 2
  %conv8.6 = uitofp i8 %37 to double
  %mul9.6 = fmul double %conv8.6, 2.126000e-01
  %conv11.6 = uitofp i8 %39 to double
  %mul12.6 = fmul double %conv11.6, 7.152000e-01
  %add13.6 = fadd double %mul9.6, %mul12.6
  %conv15.6 = uitofp i8 %41 to double
  %mul16.6 = fmul double %conv15.6, 7.220000e-02
  %add17.6 = fadd double %add13.6, %mul16.6
  %conv18.6 = fptoui double %add17.6 to i8
  %conv19.6 = uitofp i8 %conv18.6 to float
  %arrayidx21.6 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.5
  store volatile float %conv19.6, float* %arrayidx21.6, align 8
  %indvars.iv.next.6 = or i64 %indvars.iv, 7
  %42 = mul nuw nsw i64 %indvars.iv.next.6, 3
  %arrayidx.7 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %42
  %43 = load volatile i8, i8* %arrayidx.7, align 1
  %44 = add nuw nsw i64 %42, 1
  %arrayidx3.7 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %44
  %45 = load volatile i8, i8* %arrayidx3.7, align 1
  %46 = add nuw nsw i64 %42, 2
  %arrayidx6.7 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %46
  %47 = load volatile i8, i8* %arrayidx6.7, align 1
  %conv8.7 = uitofp i8 %43 to double
  %mul9.7 = fmul double %conv8.7, 2.126000e-01
  %conv11.7 = uitofp i8 %45 to double
  %mul12.7 = fmul double %conv11.7, 7.152000e-01
  %add13.7 = fadd double %mul9.7, %mul12.7
  %conv15.7 = uitofp i8 %47 to double
  %mul16.7 = fmul double %conv15.7, 7.220000e-02
  %add17.7 = fadd double %add13.7, %mul16.7
  %conv18.7 = fptoui double %add17.7 to i8
  %conv19.7 = uitofp i8 %conv18.7 to float
  %arrayidx21.7 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.6
  store volatile float %conv19.7, float* %arrayidx21.7, align 4
  %indvars.iv.next.7 = or i64 %indvars.iv, 8
  %48 = mul nuw nsw i64 %indvars.iv.next.7, 3
  %arrayidx.8 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %48
  %49 = load volatile i8, i8* %arrayidx.8, align 8
  %50 = or i64 %48, 1
  %arrayidx3.8 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %50
  %51 = load volatile i8, i8* %arrayidx3.8, align 1
  %52 = or i64 %48, 2
  %arrayidx6.8 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %52
  %53 = load volatile i8, i8* %arrayidx6.8, align 2
  %conv8.8 = uitofp i8 %49 to double
  %mul9.8 = fmul double %conv8.8, 2.126000e-01
  %conv11.8 = uitofp i8 %51 to double
  %mul12.8 = fmul double %conv11.8, 7.152000e-01
  %add13.8 = fadd double %mul9.8, %mul12.8
  %conv15.8 = uitofp i8 %53 to double
  %mul16.8 = fmul double %conv15.8, 7.220000e-02
  %add17.8 = fadd double %add13.8, %mul16.8
  %conv18.8 = fptoui double %add17.8 to i8
  %conv19.8 = uitofp i8 %conv18.8 to float
  %arrayidx21.8 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.7
  store volatile float %conv19.8, float* %arrayidx21.8, align 16
  %indvars.iv.next.8 = or i64 %indvars.iv, 9
  %54 = mul nuw nsw i64 %indvars.iv.next.8, 3
  %arrayidx.9 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %54
  %55 = load volatile i8, i8* %arrayidx.9, align 1
  %56 = add nuw nsw i64 %54, 1
  %arrayidx3.9 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %56
  %57 = load volatile i8, i8* %arrayidx3.9, align 1
  %58 = add nuw nsw i64 %54, 2
  %arrayidx6.9 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %58
  %59 = load volatile i8, i8* %arrayidx6.9, align 1
  %conv8.9 = uitofp i8 %55 to double
  %mul9.9 = fmul double %conv8.9, 2.126000e-01
  %conv11.9 = uitofp i8 %57 to double
  %mul12.9 = fmul double %conv11.9, 7.152000e-01
  %add13.9 = fadd double %mul9.9, %mul12.9
  %conv15.9 = uitofp i8 %59 to double
  %mul16.9 = fmul double %conv15.9, 7.220000e-02
  %add17.9 = fadd double %add13.9, %mul16.9
  %conv18.9 = fptoui double %add17.9 to i8
  %conv19.9 = uitofp i8 %conv18.9 to float
  %arrayidx21.9 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.8
  store volatile float %conv19.9, float* %arrayidx21.9, align 4
  %indvars.iv.next.9 = or i64 %indvars.iv, 10
  %60 = mul nuw nsw i64 %indvars.iv.next.9, 3
  %arrayidx.10 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %60
  %61 = load volatile i8, i8* %arrayidx.10, align 2
  %62 = or i64 %60, 1
  %arrayidx3.10 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %62
  %63 = load volatile i8, i8* %arrayidx3.10, align 1
  %64 = add nuw nsw i64 %60, 2
  %arrayidx6.10 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %64
  %65 = load volatile i8, i8* %arrayidx6.10, align 2
  %conv8.10 = uitofp i8 %61 to double
  %mul9.10 = fmul double %conv8.10, 2.126000e-01
  %conv11.10 = uitofp i8 %63 to double
  %mul12.10 = fmul double %conv11.10, 7.152000e-01
  %add13.10 = fadd double %mul9.10, %mul12.10
  %conv15.10 = uitofp i8 %65 to double
  %mul16.10 = fmul double %conv15.10, 7.220000e-02
  %add17.10 = fadd double %add13.10, %mul16.10
  %conv18.10 = fptoui double %add17.10 to i8
  %conv19.10 = uitofp i8 %conv18.10 to float
  %arrayidx21.10 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.9
  store volatile float %conv19.10, float* %arrayidx21.10, align 8
  %indvars.iv.next.10 = or i64 %indvars.iv, 11
  %66 = mul nuw nsw i64 %indvars.iv.next.10, 3
  %arrayidx.11 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %66
  %67 = load volatile i8, i8* %arrayidx.11, align 1
  %68 = add nuw nsw i64 %66, 1
  %arrayidx3.11 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %68
  %69 = load volatile i8, i8* %arrayidx3.11, align 1
  %70 = add nuw nsw i64 %66, 2
  %arrayidx6.11 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %70
  %71 = load volatile i8, i8* %arrayidx6.11, align 1
  %conv8.11 = uitofp i8 %67 to double
  %mul9.11 = fmul double %conv8.11, 2.126000e-01
  %conv11.11 = uitofp i8 %69 to double
  %mul12.11 = fmul double %conv11.11, 7.152000e-01
  %add13.11 = fadd double %mul9.11, %mul12.11
  %conv15.11 = uitofp i8 %71 to double
  %mul16.11 = fmul double %conv15.11, 7.220000e-02
  %add17.11 = fadd double %add13.11, %mul16.11
  %conv18.11 = fptoui double %add17.11 to i8
  %conv19.11 = uitofp i8 %conv18.11 to float
  %arrayidx21.11 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.10
  store volatile float %conv19.11, float* %arrayidx21.11, align 4
  %indvars.iv.next.11 = or i64 %indvars.iv, 12
  %72 = mul nuw nsw i64 %indvars.iv.next.11, 3
  %arrayidx.12 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %72
  %73 = load volatile i8, i8* %arrayidx.12, align 4
  %74 = or i64 %72, 1
  %arrayidx3.12 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %74
  %75 = load volatile i8, i8* %arrayidx3.12, align 1
  %76 = or i64 %72, 2
  %arrayidx6.12 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %76
  %77 = load volatile i8, i8* %arrayidx6.12, align 2
  %conv8.12 = uitofp i8 %73 to double
  %mul9.12 = fmul double %conv8.12, 2.126000e-01
  %conv11.12 = uitofp i8 %75 to double
  %mul12.12 = fmul double %conv11.12, 7.152000e-01
  %add13.12 = fadd double %mul9.12, %mul12.12
  %conv15.12 = uitofp i8 %77 to double
  %mul16.12 = fmul double %conv15.12, 7.220000e-02
  %add17.12 = fadd double %add13.12, %mul16.12
  %conv18.12 = fptoui double %add17.12 to i8
  %conv19.12 = uitofp i8 %conv18.12 to float
  %arrayidx21.12 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.11
  store volatile float %conv19.12, float* %arrayidx21.12, align 16
  %indvars.iv.next.12 = or i64 %indvars.iv, 13
  %78 = mul nuw nsw i64 %indvars.iv.next.12, 3
  %arrayidx.13 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %78
  %79 = load volatile i8, i8* %arrayidx.13, align 1
  %80 = add nuw nsw i64 %78, 1
  %arrayidx3.13 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %80
  %81 = load volatile i8, i8* %arrayidx3.13, align 1
  %82 = add nuw nsw i64 %78, 2
  %arrayidx6.13 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %82
  %83 = load volatile i8, i8* %arrayidx6.13, align 1
  %conv8.13 = uitofp i8 %79 to double
  %mul9.13 = fmul double %conv8.13, 2.126000e-01
  %conv11.13 = uitofp i8 %81 to double
  %mul12.13 = fmul double %conv11.13, 7.152000e-01
  %add13.13 = fadd double %mul9.13, %mul12.13
  %conv15.13 = uitofp i8 %83 to double
  %mul16.13 = fmul double %conv15.13, 7.220000e-02
  %add17.13 = fadd double %add13.13, %mul16.13
  %conv18.13 = fptoui double %add17.13 to i8
  %conv19.13 = uitofp i8 %conv18.13 to float
  %arrayidx21.13 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.12
  store volatile float %conv19.13, float* %arrayidx21.13, align 4
  %indvars.iv.next.13 = or i64 %indvars.iv, 14
  %84 = mul nuw nsw i64 %indvars.iv.next.13, 3
  %arrayidx.14 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %84
  %85 = load volatile i8, i8* %arrayidx.14, align 2
  %86 = or i64 %84, 1
  %arrayidx3.14 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %86
  %87 = load volatile i8, i8* %arrayidx3.14, align 1
  %88 = add nuw nsw i64 %84, 2
  %arrayidx6.14 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %88
  %89 = load volatile i8, i8* %arrayidx6.14, align 2
  %conv8.14 = uitofp i8 %85 to double
  %mul9.14 = fmul double %conv8.14, 2.126000e-01
  %conv11.14 = uitofp i8 %87 to double
  %mul12.14 = fmul double %conv11.14, 7.152000e-01
  %add13.14 = fadd double %mul9.14, %mul12.14
  %conv15.14 = uitofp i8 %89 to double
  %mul16.14 = fmul double %conv15.14, 7.220000e-02
  %add17.14 = fadd double %add13.14, %mul16.14
  %conv18.14 = fptoui double %add17.14 to i8
  %conv19.14 = uitofp i8 %conv18.14 to float
  %arrayidx21.14 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.13
  store volatile float %conv19.14, float* %arrayidx21.14, align 8
  %indvars.iv.next.14 = or i64 %indvars.iv, 15
  %90 = mul nuw nsw i64 %indvars.iv.next.14, 3
  %arrayidx.15 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %90
  %91 = load volatile i8, i8* %arrayidx.15, align 1
  %92 = add nuw nsw i64 %90, 1
  %arrayidx3.15 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %92
  %93 = load volatile i8, i8* %arrayidx3.15, align 1
  %94 = add nuw nsw i64 %90, 2
  %arrayidx6.15 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %94
  %95 = load volatile i8, i8* %arrayidx6.15, align 1
  %conv8.15 = uitofp i8 %91 to double
  %mul9.15 = fmul double %conv8.15, 2.126000e-01
  %conv11.15 = uitofp i8 %93 to double
  %mul12.15 = fmul double %conv11.15, 7.152000e-01
  %add13.15 = fadd double %mul9.15, %mul12.15
  %conv15.15 = uitofp i8 %95 to double
  %mul16.15 = fmul double %conv15.15, 7.220000e-02
  %add17.15 = fadd double %add13.15, %mul16.15
  %conv18.15 = fptoui double %add17.15 to i8
  %conv19.15 = uitofp i8 %conv18.15 to float
  %arrayidx21.15 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.14
  store volatile float %conv19.15, float* %arrayidx21.15, align 4
  %indvars.iv.next.15 = or i64 %indvars.iv, 16
  %96 = mul nuw nsw i64 %indvars.iv.next.15, 3
  %arrayidx.16 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %96
  %97 = load volatile i8, i8* %arrayidx.16, align 16
  %98 = or i64 %96, 1
  %arrayidx3.16 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %98
  %99 = load volatile i8, i8* %arrayidx3.16, align 1
  %100 = or i64 %96, 2
  %arrayidx6.16 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %100
  %101 = load volatile i8, i8* %arrayidx6.16, align 2
  %conv8.16 = uitofp i8 %97 to double
  %mul9.16 = fmul double %conv8.16, 2.126000e-01
  %conv11.16 = uitofp i8 %99 to double
  %mul12.16 = fmul double %conv11.16, 7.152000e-01
  %add13.16 = fadd double %mul9.16, %mul12.16
  %conv15.16 = uitofp i8 %101 to double
  %mul16.16 = fmul double %conv15.16, 7.220000e-02
  %add17.16 = fadd double %add13.16, %mul16.16
  %conv18.16 = fptoui double %add17.16 to i8
  %conv19.16 = uitofp i8 %conv18.16 to float
  %arrayidx21.16 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.15
  store volatile float %conv19.16, float* %arrayidx21.16, align 16
  %indvars.iv.next.16 = or i64 %indvars.iv, 17
  %102 = mul nuw nsw i64 %indvars.iv.next.16, 3
  %arrayidx.17 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %102
  %103 = load volatile i8, i8* %arrayidx.17, align 1
  %104 = add nuw nsw i64 %102, 1
  %arrayidx3.17 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %104
  %105 = load volatile i8, i8* %arrayidx3.17, align 1
  %106 = add nuw nsw i64 %102, 2
  %arrayidx6.17 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %106
  %107 = load volatile i8, i8* %arrayidx6.17, align 1
  %conv8.17 = uitofp i8 %103 to double
  %mul9.17 = fmul double %conv8.17, 2.126000e-01
  %conv11.17 = uitofp i8 %105 to double
  %mul12.17 = fmul double %conv11.17, 7.152000e-01
  %add13.17 = fadd double %mul9.17, %mul12.17
  %conv15.17 = uitofp i8 %107 to double
  %mul16.17 = fmul double %conv15.17, 7.220000e-02
  %add17.17 = fadd double %add13.17, %mul16.17
  %conv18.17 = fptoui double %add17.17 to i8
  %conv19.17 = uitofp i8 %conv18.17 to float
  %arrayidx21.17 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.16
  store volatile float %conv19.17, float* %arrayidx21.17, align 4
  %indvars.iv.next.17 = or i64 %indvars.iv, 18
  %108 = mul nuw nsw i64 %indvars.iv.next.17, 3
  %arrayidx.18 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %108
  %109 = load volatile i8, i8* %arrayidx.18, align 2
  %110 = or i64 %108, 1
  %arrayidx3.18 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %110
  %111 = load volatile i8, i8* %arrayidx3.18, align 1
  %112 = add nuw nsw i64 %108, 2
  %arrayidx6.18 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %112
  %113 = load volatile i8, i8* %arrayidx6.18, align 2
  %conv8.18 = uitofp i8 %109 to double
  %mul9.18 = fmul double %conv8.18, 2.126000e-01
  %conv11.18 = uitofp i8 %111 to double
  %mul12.18 = fmul double %conv11.18, 7.152000e-01
  %add13.18 = fadd double %mul9.18, %mul12.18
  %conv15.18 = uitofp i8 %113 to double
  %mul16.18 = fmul double %conv15.18, 7.220000e-02
  %add17.18 = fadd double %add13.18, %mul16.18
  %conv18.18 = fptoui double %add17.18 to i8
  %conv19.18 = uitofp i8 %conv18.18 to float
  %arrayidx21.18 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.17
  store volatile float %conv19.18, float* %arrayidx21.18, align 8
  %indvars.iv.next.18 = or i64 %indvars.iv, 19
  %114 = mul nuw nsw i64 %indvars.iv.next.18, 3
  %arrayidx.19 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %114
  %115 = load volatile i8, i8* %arrayidx.19, align 1
  %116 = add nuw nsw i64 %114, 1
  %arrayidx3.19 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %116
  %117 = load volatile i8, i8* %arrayidx3.19, align 1
  %118 = add nuw nsw i64 %114, 2
  %arrayidx6.19 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %118
  %119 = load volatile i8, i8* %arrayidx6.19, align 1
  %conv8.19 = uitofp i8 %115 to double
  %mul9.19 = fmul double %conv8.19, 2.126000e-01
  %conv11.19 = uitofp i8 %117 to double
  %mul12.19 = fmul double %conv11.19, 7.152000e-01
  %add13.19 = fadd double %mul9.19, %mul12.19
  %conv15.19 = uitofp i8 %119 to double
  %mul16.19 = fmul double %conv15.19, 7.220000e-02
  %add17.19 = fadd double %add13.19, %mul16.19
  %conv18.19 = fptoui double %add17.19 to i8
  %conv19.19 = uitofp i8 %conv18.19 to float
  %arrayidx21.19 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.18
  store volatile float %conv19.19, float* %arrayidx21.19, align 4
  %indvars.iv.next.19 = or i64 %indvars.iv, 20
  %120 = mul nuw nsw i64 %indvars.iv.next.19, 3
  %arrayidx.20 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %120
  %121 = load volatile i8, i8* %arrayidx.20, align 4
  %122 = or i64 %120, 1
  %arrayidx3.20 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %122
  %123 = load volatile i8, i8* %arrayidx3.20, align 1
  %124 = or i64 %120, 2
  %arrayidx6.20 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %124
  %125 = load volatile i8, i8* %arrayidx6.20, align 2
  %conv8.20 = uitofp i8 %121 to double
  %mul9.20 = fmul double %conv8.20, 2.126000e-01
  %conv11.20 = uitofp i8 %123 to double
  %mul12.20 = fmul double %conv11.20, 7.152000e-01
  %add13.20 = fadd double %mul9.20, %mul12.20
  %conv15.20 = uitofp i8 %125 to double
  %mul16.20 = fmul double %conv15.20, 7.220000e-02
  %add17.20 = fadd double %add13.20, %mul16.20
  %conv18.20 = fptoui double %add17.20 to i8
  %conv19.20 = uitofp i8 %conv18.20 to float
  %arrayidx21.20 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.19
  store volatile float %conv19.20, float* %arrayidx21.20, align 16
  %indvars.iv.next.20 = or i64 %indvars.iv, 21
  %126 = mul nuw nsw i64 %indvars.iv.next.20, 3
  %arrayidx.21 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %126
  %127 = load volatile i8, i8* %arrayidx.21, align 1
  %128 = add nuw nsw i64 %126, 1
  %arrayidx3.21 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %128
  %129 = load volatile i8, i8* %arrayidx3.21, align 1
  %130 = add nuw nsw i64 %126, 2
  %arrayidx6.21 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %130
  %131 = load volatile i8, i8* %arrayidx6.21, align 1
  %conv8.21 = uitofp i8 %127 to double
  %mul9.21 = fmul double %conv8.21, 2.126000e-01
  %conv11.21 = uitofp i8 %129 to double
  %mul12.21 = fmul double %conv11.21, 7.152000e-01
  %add13.21 = fadd double %mul9.21, %mul12.21
  %conv15.21 = uitofp i8 %131 to double
  %mul16.21 = fmul double %conv15.21, 7.220000e-02
  %add17.21 = fadd double %add13.21, %mul16.21
  %conv18.21 = fptoui double %add17.21 to i8
  %conv19.21 = uitofp i8 %conv18.21 to float
  %arrayidx21.21 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.20
  store volatile float %conv19.21, float* %arrayidx21.21, align 4
  %indvars.iv.next.21 = or i64 %indvars.iv, 22
  %132 = mul nuw nsw i64 %indvars.iv.next.21, 3
  %arrayidx.22 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %132
  %133 = load volatile i8, i8* %arrayidx.22, align 2
  %134 = or i64 %132, 1
  %arrayidx3.22 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %134
  %135 = load volatile i8, i8* %arrayidx3.22, align 1
  %136 = add nuw nsw i64 %132, 2
  %arrayidx6.22 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %136
  %137 = load volatile i8, i8* %arrayidx6.22, align 2
  %conv8.22 = uitofp i8 %133 to double
  %mul9.22 = fmul double %conv8.22, 2.126000e-01
  %conv11.22 = uitofp i8 %135 to double
  %mul12.22 = fmul double %conv11.22, 7.152000e-01
  %add13.22 = fadd double %mul9.22, %mul12.22
  %conv15.22 = uitofp i8 %137 to double
  %mul16.22 = fmul double %conv15.22, 7.220000e-02
  %add17.22 = fadd double %add13.22, %mul16.22
  %conv18.22 = fptoui double %add17.22 to i8
  %conv19.22 = uitofp i8 %conv18.22 to float
  %arrayidx21.22 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.21
  store volatile float %conv19.22, float* %arrayidx21.22, align 8
  %indvars.iv.next.22 = or i64 %indvars.iv, 23
  %138 = mul nuw nsw i64 %indvars.iv.next.22, 3
  %arrayidx.23 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %138
  %139 = load volatile i8, i8* %arrayidx.23, align 1
  %140 = add nuw nsw i64 %138, 1
  %arrayidx3.23 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %140
  %141 = load volatile i8, i8* %arrayidx3.23, align 1
  %142 = add nuw nsw i64 %138, 2
  %arrayidx6.23 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %142
  %143 = load volatile i8, i8* %arrayidx6.23, align 1
  %conv8.23 = uitofp i8 %139 to double
  %mul9.23 = fmul double %conv8.23, 2.126000e-01
  %conv11.23 = uitofp i8 %141 to double
  %mul12.23 = fmul double %conv11.23, 7.152000e-01
  %add13.23 = fadd double %mul9.23, %mul12.23
  %conv15.23 = uitofp i8 %143 to double
  %mul16.23 = fmul double %conv15.23, 7.220000e-02
  %add17.23 = fadd double %add13.23, %mul16.23
  %conv18.23 = fptoui double %add17.23 to i8
  %conv19.23 = uitofp i8 %conv18.23 to float
  %arrayidx21.23 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.22
  store volatile float %conv19.23, float* %arrayidx21.23, align 4
  %indvars.iv.next.23 = or i64 %indvars.iv, 24
  %144 = mul nuw nsw i64 %indvars.iv.next.23, 3
  %arrayidx.24 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %144
  %145 = load volatile i8, i8* %arrayidx.24, align 8
  %146 = or i64 %144, 1
  %arrayidx3.24 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %146
  %147 = load volatile i8, i8* %arrayidx3.24, align 1
  %148 = or i64 %144, 2
  %arrayidx6.24 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %148
  %149 = load volatile i8, i8* %arrayidx6.24, align 2
  %conv8.24 = uitofp i8 %145 to double
  %mul9.24 = fmul double %conv8.24, 2.126000e-01
  %conv11.24 = uitofp i8 %147 to double
  %mul12.24 = fmul double %conv11.24, 7.152000e-01
  %add13.24 = fadd double %mul9.24, %mul12.24
  %conv15.24 = uitofp i8 %149 to double
  %mul16.24 = fmul double %conv15.24, 7.220000e-02
  %add17.24 = fadd double %add13.24, %mul16.24
  %conv18.24 = fptoui double %add17.24 to i8
  %conv19.24 = uitofp i8 %conv18.24 to float
  %arrayidx21.24 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.23
  store volatile float %conv19.24, float* %arrayidx21.24, align 16
  %indvars.iv.next.24 = or i64 %indvars.iv, 25
  %150 = mul nuw nsw i64 %indvars.iv.next.24, 3
  %arrayidx.25 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %150
  %151 = load volatile i8, i8* %arrayidx.25, align 1
  %152 = add nuw nsw i64 %150, 1
  %arrayidx3.25 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %152
  %153 = load volatile i8, i8* %arrayidx3.25, align 1
  %154 = add nuw nsw i64 %150, 2
  %arrayidx6.25 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %154
  %155 = load volatile i8, i8* %arrayidx6.25, align 1
  %conv8.25 = uitofp i8 %151 to double
  %mul9.25 = fmul double %conv8.25, 2.126000e-01
  %conv11.25 = uitofp i8 %153 to double
  %mul12.25 = fmul double %conv11.25, 7.152000e-01
  %add13.25 = fadd double %mul9.25, %mul12.25
  %conv15.25 = uitofp i8 %155 to double
  %mul16.25 = fmul double %conv15.25, 7.220000e-02
  %add17.25 = fadd double %add13.25, %mul16.25
  %conv18.25 = fptoui double %add17.25 to i8
  %conv19.25 = uitofp i8 %conv18.25 to float
  %arrayidx21.25 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.24
  store volatile float %conv19.25, float* %arrayidx21.25, align 4
  %indvars.iv.next.25 = or i64 %indvars.iv, 26
  %156 = mul nuw nsw i64 %indvars.iv.next.25, 3
  %arrayidx.26 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %156
  %157 = load volatile i8, i8* %arrayidx.26, align 2
  %158 = or i64 %156, 1
  %arrayidx3.26 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %158
  %159 = load volatile i8, i8* %arrayidx3.26, align 1
  %160 = add nuw nsw i64 %156, 2
  %arrayidx6.26 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %160
  %161 = load volatile i8, i8* %arrayidx6.26, align 2
  %conv8.26 = uitofp i8 %157 to double
  %mul9.26 = fmul double %conv8.26, 2.126000e-01
  %conv11.26 = uitofp i8 %159 to double
  %mul12.26 = fmul double %conv11.26, 7.152000e-01
  %add13.26 = fadd double %mul9.26, %mul12.26
  %conv15.26 = uitofp i8 %161 to double
  %mul16.26 = fmul double %conv15.26, 7.220000e-02
  %add17.26 = fadd double %add13.26, %mul16.26
  %conv18.26 = fptoui double %add17.26 to i8
  %conv19.26 = uitofp i8 %conv18.26 to float
  %arrayidx21.26 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.25
  store volatile float %conv19.26, float* %arrayidx21.26, align 8
  %indvars.iv.next.26 = or i64 %indvars.iv, 27
  %162 = mul nuw nsw i64 %indvars.iv.next.26, 3
  %arrayidx.27 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %162
  %163 = load volatile i8, i8* %arrayidx.27, align 1
  %164 = add nuw nsw i64 %162, 1
  %arrayidx3.27 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %164
  %165 = load volatile i8, i8* %arrayidx3.27, align 1
  %166 = add nuw nsw i64 %162, 2
  %arrayidx6.27 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %166
  %167 = load volatile i8, i8* %arrayidx6.27, align 1
  %conv8.27 = uitofp i8 %163 to double
  %mul9.27 = fmul double %conv8.27, 2.126000e-01
  %conv11.27 = uitofp i8 %165 to double
  %mul12.27 = fmul double %conv11.27, 7.152000e-01
  %add13.27 = fadd double %mul9.27, %mul12.27
  %conv15.27 = uitofp i8 %167 to double
  %mul16.27 = fmul double %conv15.27, 7.220000e-02
  %add17.27 = fadd double %add13.27, %mul16.27
  %conv18.27 = fptoui double %add17.27 to i8
  %conv19.27 = uitofp i8 %conv18.27 to float
  %arrayidx21.27 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.26
  store volatile float %conv19.27, float* %arrayidx21.27, align 4
  %indvars.iv.next.27 = or i64 %indvars.iv, 28
  %168 = mul nuw nsw i64 %indvars.iv.next.27, 3
  %arrayidx.28 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %168
  %169 = load volatile i8, i8* %arrayidx.28, align 4
  %170 = or i64 %168, 1
  %arrayidx3.28 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %170
  %171 = load volatile i8, i8* %arrayidx3.28, align 1
  %172 = or i64 %168, 2
  %arrayidx6.28 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %172
  %173 = load volatile i8, i8* %arrayidx6.28, align 2
  %conv8.28 = uitofp i8 %169 to double
  %mul9.28 = fmul double %conv8.28, 2.126000e-01
  %conv11.28 = uitofp i8 %171 to double
  %mul12.28 = fmul double %conv11.28, 7.152000e-01
  %add13.28 = fadd double %mul9.28, %mul12.28
  %conv15.28 = uitofp i8 %173 to double
  %mul16.28 = fmul double %conv15.28, 7.220000e-02
  %add17.28 = fadd double %add13.28, %mul16.28
  %conv18.28 = fptoui double %add17.28 to i8
  %conv19.28 = uitofp i8 %conv18.28 to float
  %arrayidx21.28 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.27
  store volatile float %conv19.28, float* %arrayidx21.28, align 16
  %indvars.iv.next.28 = or i64 %indvars.iv, 29
  %174 = mul nuw nsw i64 %indvars.iv.next.28, 3
  %arrayidx.29 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %174
  %175 = load volatile i8, i8* %arrayidx.29, align 1
  %176 = add nuw nsw i64 %174, 1
  %arrayidx3.29 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %176
  %177 = load volatile i8, i8* %arrayidx3.29, align 1
  %178 = add nuw nsw i64 %174, 2
  %arrayidx6.29 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %178
  %179 = load volatile i8, i8* %arrayidx6.29, align 1
  %conv8.29 = uitofp i8 %175 to double
  %mul9.29 = fmul double %conv8.29, 2.126000e-01
  %conv11.29 = uitofp i8 %177 to double
  %mul12.29 = fmul double %conv11.29, 7.152000e-01
  %add13.29 = fadd double %mul9.29, %mul12.29
  %conv15.29 = uitofp i8 %179 to double
  %mul16.29 = fmul double %conv15.29, 7.220000e-02
  %add17.29 = fadd double %add13.29, %mul16.29
  %conv18.29 = fptoui double %add17.29 to i8
  %conv19.29 = uitofp i8 %conv18.29 to float
  %arrayidx21.29 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.28
  store volatile float %conv19.29, float* %arrayidx21.29, align 4
  %indvars.iv.next.29 = or i64 %indvars.iv, 30
  %180 = mul nuw nsw i64 %indvars.iv.next.29, 3
  %arrayidx.30 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %180
  %181 = load volatile i8, i8* %arrayidx.30, align 2
  %182 = or i64 %180, 1
  %arrayidx3.30 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %182
  %183 = load volatile i8, i8* %arrayidx3.30, align 1
  %184 = add nuw nsw i64 %180, 2
  %arrayidx6.30 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %184
  %185 = load volatile i8, i8* %arrayidx6.30, align 2
  %conv8.30 = uitofp i8 %181 to double
  %mul9.30 = fmul double %conv8.30, 2.126000e-01
  %conv11.30 = uitofp i8 %183 to double
  %mul12.30 = fmul double %conv11.30, 7.152000e-01
  %add13.30 = fadd double %mul9.30, %mul12.30
  %conv15.30 = uitofp i8 %185 to double
  %mul16.30 = fmul double %conv15.30, 7.220000e-02
  %add17.30 = fadd double %add13.30, %mul16.30
  %conv18.30 = fptoui double %add17.30 to i8
  %conv19.30 = uitofp i8 %conv18.30 to float
  %arrayidx21.30 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.29
  store volatile float %conv19.30, float* %arrayidx21.30, align 8
  %indvars.iv.next.30 = or i64 %indvars.iv, 31
  %186 = mul nuw nsw i64 %indvars.iv.next.30, 3
  %arrayidx.31 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %186
  %187 = load volatile i8, i8* %arrayidx.31, align 1
  %188 = add nuw nsw i64 %186, 1
  %arrayidx3.31 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %188
  %189 = load volatile i8, i8* %arrayidx3.31, align 1
  %190 = add nuw nsw i64 %186, 2
  %arrayidx6.31 = getelementptr inbounds i8, i8* inttoptr (i64 537923600 to i8*), i64 %190
  %191 = load volatile i8, i8* %arrayidx6.31, align 1
  %conv8.31 = uitofp i8 %187 to double
  %mul9.31 = fmul double %conv8.31, 2.126000e-01
  %conv11.31 = uitofp i8 %189 to double
  %mul12.31 = fmul double %conv11.31, 7.152000e-01
  %add13.31 = fadd double %mul9.31, %mul12.31
  %conv15.31 = uitofp i8 %191 to double
  %mul16.31 = fmul double %conv15.31, 7.220000e-02
  %add17.31 = fadd double %add13.31, %mul16.31
  %conv18.31 = fptoui double %add17.31 to i8
  %conv19.31 = uitofp i8 %conv18.31 to float
  %arrayidx21.31 = getelementptr inbounds float, float* inttoptr (i64 537927696 to float*), i64 %indvars.iv.next.30
  store volatile float %conv19.31, float* %arrayidx21.31, align 4
  %indvars.iv.next.31 = add nsw i64 %indvars.iv, 32
  %exitcond.31 = icmp eq i64 %indvars.iv.next.31, 4096
  br i1 %exitcond.31, label %for.end, label %for.body, !llvm.loop !1

for.end:                                          ; preds = %for.body
  ret void
}

attributes #0 = { norecurse nounwind uwtable "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+fxsr,+mmx,+sse,+sse2" "unsafe-fp-math"="false" "use-soft-float"="false" }

!llvm.ident = !{!0}

!0 = !{!"clang version 3.8.1 "}
!1 = distinct !{!1, !2}
!2 = !{!"llvm.loop.unroll.disable"}
